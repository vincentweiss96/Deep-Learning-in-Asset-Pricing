{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.optimizers import SGD, Adam, RMSprop, Adamax, Adadelta\n",
    "from keras.models import Model, load_model\n",
    "from keras import Input\n",
    "from keras.layers import Dense, Dropout, Multiply, concatenate, LeakyReLU\n",
    "from matplotlib import pyplot as plt\n",
    "import keras.backend as K\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.preprocessing import quantile_transform\n",
    "import psycopg2\n",
    "import missingno as msno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Definition\n",
    "def custom_loss(y_true,y_pred):\n",
    "        # M * R * G\n",
    "        # (1-w *R) * R * G\n",
    "        weights = y_pred[:,0]\n",
    "        g_i = y_pred[:,1]\n",
    "        excess_returns = y_true\n",
    "        M = 1-K.sum(weights * excess_returns)\n",
    "        custom_loss_value = K.mean((M * excess_returns * g_i)**2)\n",
    "        return custom_loss_value\n",
    "# model = load_model('model.h5', \n",
    "#                    custom_objects={'loss': asymmetric_loss(alpha)})\n",
    "\n",
    "def loss_arguments(Tickers, i):\n",
    "    observation_weights =\\\n",
    "    Tickers[\"Count\"].iloc[i*128:i*128+128]/\\\n",
    "    Tickers[\"Count\"].iloc[i*128:i*128+128].sum()\n",
    "    def custom_loss(y_true,y_pred):\n",
    "        # M * R * G\n",
    "        # (1-w *R) * R * G\n",
    "        weights = y_pred[:,0]\n",
    "        g_i = y_pred[:,1]\n",
    "        excess_returns = y_true\n",
    "        M = 1-K.sum(weights * excess_returns)\n",
    "        custom_loss_value = K.mean((M * excess_returns * g_i)**2)\n",
    "        return custom_loss_value\n",
    "    return custom_loss_value\n",
    "\n",
    "\n",
    "def custom_loss_adverse(y_true, y_pred):\n",
    "    # M * R * G\n",
    "    # (1-w *R) * R * G\n",
    "    weights = y_pred[:,0]\n",
    "    g_i = y_pred[:,1]\n",
    "    excess_returns = y_true\n",
    "    M = 1-K.sum(weights * excess_returns)\n",
    "    custom_loss_value = - K.mean((M * excess_returns * g_i)**2)\n",
    "    return custom_loss_value\n",
    "\n",
    "class CustomCallback(Callback):\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        global i\n",
    "        global Tickers\n",
    "        \n",
    "        observation_weights_ =\\\n",
    "        Tickers[\"Count\"].iloc[i*128:i*128+128]/\\\n",
    "        Tickers[\"Count\"].iloc[i*128:i*128+128].sum()        \n",
    "        i = i+1\n",
    "        #print(observation_weights_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(438,)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (491,)\n",
    "network=None\n",
    "opt=Adamax(lr=0.001)\n",
    "    \n",
    "    \n",
    "Input_object = Input(shape =input_shape, name='Input', dtype='float32')\n",
    "Dense1 = Dense(50, activation='sigmoid', name='Dense1', input_shape=input_shape)(Input_object)\n",
    "Dropout1= Dropout(0.05)(Dense1)\n",
    "Dense2 = Dense(50, activation='sigmoid', name='Dense2')(Dropout1)\n",
    "Dense3 = Dense(1, activation='sigmoid', name='Dense3')(Dense2)\n",
    "#LeakyReLU\n",
    "G_1 = Dense(50, activation='sigmoid', name='G_1', input_shape=input_shape)(Input_object)\n",
    "\n",
    "# Sigmoid works\n",
    "G_2 = Dense(1, activation='sigmoid', name='G_2')(G_1)\n",
    "\n",
    "Out = concatenate([Dense3, G_2], name='Output')\n",
    "\n",
    "# X_features_set = X\n",
    "# Y_features_set = np.array(Y)\n",
    "# x_features_set = X[0:1000]\n",
    "# y_features_set = np.array(Y[0:1000])\n",
    "network = Model([Input_object], Out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got column_names\n",
      "got macro_column_names\n",
      "got macro_data\n"
     ]
    }
   ],
   "source": [
    "# database cols: needed only once\n",
    "\n",
    "conn = psycopg2.connect(\"dbname=Kelly_2017_2020 user=postgres password=password\")\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute(\"SELECT column_name from information_schema.columns WHERE (table_name = 'data');\")\n",
    "column_names = cur.fetchall()\n",
    "print(\"got column_names\")\n",
    "\n",
    "cur.execute(\"SELECT column_name from information_schema.columns WHERE (table_name = 'macro');\")\n",
    "macro_column_names = cur.fetchall()\n",
    "print(\"got macro_column_names\")\n",
    "\n",
    "cur.execute(\"SELECT * From macro;\")\n",
    "macro_data = cur.fetchall()\n",
    "macro_data = pd.DataFrame(macro_data, columns=pd.DataFrame(macro_column_names)[0])\n",
    "\n",
    "print(\"got macro_data\")\n",
    "\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables=pd.read_excel(\"PATH\\\\Output\\\\variables_small.xlsx\",\\\n",
    "                  sheet_name = \"variables\", usecols = \"A:B\")\n",
    "identifiers=variables[(variables[\"TYPE\"]==\"identifier\") | (variables[\"TYPE\"]==\"observation identifier\")]\n",
    "identifiers=list(identifiers[\"VARIABLE\"])\n",
    "identifiers.remove(\"eom\")\n",
    "\n",
    "delete_cols = pd.read_csv(\"Networks\\delete_cols.csv\")\n",
    "delete_cols =delete_cols[\"0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_cols = val_batch.loc[:,val_batch.isna().sum()>val_batch.isna().sum().quantile(0.3)].columns.values\n",
    "pd.DataFrame(delete_cols).to_csv(\"Networks\\delete_cols.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2569f576f08>"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plist = []\n",
    "vlist = []\n",
    "for i in range (1,100):\n",
    "    \n",
    "    zz=val_batch.loc[:,val_batch.isna().sum()>val_batch.isna().sum().quantile(i/100)].columns.values\n",
    "    plist.append(i/100)\n",
    "    vlist.append(len(zz))\n",
    "    \n",
    "plt.scatter(plist,vlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_batch.drop(columns=delete_cols, inplace = True)\n",
    "val_batch=val_batch.loc[False==val_batch.isna().any(axis=1),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# once only to save time\n",
    "conn = psycopg2.connect(\"dbname=Kelly_2017_2020 user=postgres password=password\")\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute(\"SELECT * FROM data TABLESAMPLE BERNOULLI(5);\")\n",
    "val_batch = cur.fetchall()\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "val_batch = pd.DataFrame(val_batch, columns = pd.DataFrame(column_names)[0])\n",
    "val_batch.replace(\"EMPTY\", np.nan, inplace = True)\n",
    "val_batch.drop(columns=identifiers, inplace = True)\n",
    "val_batch = val_batch[False == val_batch[\"ret_exc_lead1m\"].isnull()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>481</th>\n",
       "      <th>482</th>\n",
       "      <th>483</th>\n",
       "      <th>484</th>\n",
       "      <th>485</th>\n",
       "      <th>486</th>\n",
       "      <th>487</th>\n",
       "      <th>488</th>\n",
       "      <th>489</th>\n",
       "      <th>490</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.270006</td>\n",
       "      <td>0.616268</td>\n",
       "      <td>0.614537</td>\n",
       "      <td>0.889855</td>\n",
       "      <td>0.889855</td>\n",
       "      <td>0.891098</td>\n",
       "      <td>0.900855</td>\n",
       "      <td>0.327238</td>\n",
       "      <td>0.119122</td>\n",
       "      <td>0.242768</td>\n",
       "      <td>...</td>\n",
       "      <td>0.314815</td>\n",
       "      <td>0.314815</td>\n",
       "      <td>0.619119</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.946947</td>\n",
       "      <td>0.394394</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.376877</td>\n",
       "      <td>0.277277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.083130</td>\n",
       "      <td>0.062072</td>\n",
       "      <td>0.061091</td>\n",
       "      <td>0.127694</td>\n",
       "      <td>0.127694</td>\n",
       "      <td>0.122157</td>\n",
       "      <td>0.140623</td>\n",
       "      <td>0.052177</td>\n",
       "      <td>0.078007</td>\n",
       "      <td>0.294970</td>\n",
       "      <td>...</td>\n",
       "      <td>0.314815</td>\n",
       "      <td>0.314815</td>\n",
       "      <td>0.619119</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.946947</td>\n",
       "      <td>0.394394</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.376877</td>\n",
       "      <td>0.277277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.736763</td>\n",
       "      <td>0.904569</td>\n",
       "      <td>0.901883</td>\n",
       "      <td>0.942353</td>\n",
       "      <td>0.942353</td>\n",
       "      <td>0.945712</td>\n",
       "      <td>0.949003</td>\n",
       "      <td>0.909945</td>\n",
       "      <td>0.706564</td>\n",
       "      <td>0.162829</td>\n",
       "      <td>...</td>\n",
       "      <td>0.314815</td>\n",
       "      <td>0.314815</td>\n",
       "      <td>0.619119</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.946947</td>\n",
       "      <td>0.394394</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.376877</td>\n",
       "      <td>0.277277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.872793</td>\n",
       "      <td>0.926812</td>\n",
       "      <td>0.926584</td>\n",
       "      <td>0.862798</td>\n",
       "      <td>0.862798</td>\n",
       "      <td>0.850648</td>\n",
       "      <td>0.859840</td>\n",
       "      <td>0.895038</td>\n",
       "      <td>0.788793</td>\n",
       "      <td>0.616153</td>\n",
       "      <td>...</td>\n",
       "      <td>0.314815</td>\n",
       "      <td>0.314815</td>\n",
       "      <td>0.619119</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.946947</td>\n",
       "      <td>0.394394</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.376877</td>\n",
       "      <td>0.277277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.090079</td>\n",
       "      <td>0.234805</td>\n",
       "      <td>0.232698</td>\n",
       "      <td>0.572573</td>\n",
       "      <td>0.572573</td>\n",
       "      <td>0.621191</td>\n",
       "      <td>0.589874</td>\n",
       "      <td>0.037066</td>\n",
       "      <td>0.022950</td>\n",
       "      <td>0.071108</td>\n",
       "      <td>...</td>\n",
       "      <td>0.314815</td>\n",
       "      <td>0.314815</td>\n",
       "      <td>0.619119</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.946947</td>\n",
       "      <td>0.394394</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.376877</td>\n",
       "      <td>0.277277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2329</th>\n",
       "      <td>0.793802</td>\n",
       "      <td>0.745054</td>\n",
       "      <td>0.742796</td>\n",
       "      <td>0.555385</td>\n",
       "      <td>0.555385</td>\n",
       "      <td>0.537524</td>\n",
       "      <td>0.559688</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.657090</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061562</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.938939</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.228228</td>\n",
       "      <td>0.142643</td>\n",
       "      <td>0.528529</td>\n",
       "      <td>0.802803</td>\n",
       "      <td>0.733734</td>\n",
       "      <td>0.134134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2330</th>\n",
       "      <td>0.857986</td>\n",
       "      <td>0.760795</td>\n",
       "      <td>0.758566</td>\n",
       "      <td>0.481947</td>\n",
       "      <td>0.481947</td>\n",
       "      <td>0.450607</td>\n",
       "      <td>0.505775</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.529989</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061562</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.938939</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.228228</td>\n",
       "      <td>0.142643</td>\n",
       "      <td>0.528529</td>\n",
       "      <td>0.802803</td>\n",
       "      <td>0.733734</td>\n",
       "      <td>0.134134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2331</th>\n",
       "      <td>0.964639</td>\n",
       "      <td>0.865542</td>\n",
       "      <td>0.864202</td>\n",
       "      <td>0.345782</td>\n",
       "      <td>0.345782</td>\n",
       "      <td>0.318538</td>\n",
       "      <td>0.368198</td>\n",
       "      <td>0.610694</td>\n",
       "      <td>0.761628</td>\n",
       "      <td>0.592665</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061562</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.938939</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.228228</td>\n",
       "      <td>0.142643</td>\n",
       "      <td>0.528529</td>\n",
       "      <td>0.802803</td>\n",
       "      <td>0.733734</td>\n",
       "      <td>0.134134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2332</th>\n",
       "      <td>0.442278</td>\n",
       "      <td>0.663397</td>\n",
       "      <td>0.661968</td>\n",
       "      <td>0.812899</td>\n",
       "      <td>0.812899</td>\n",
       "      <td>0.802238</td>\n",
       "      <td>0.828281</td>\n",
       "      <td>0.686671</td>\n",
       "      <td>0.507499</td>\n",
       "      <td>0.447593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061562</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.938939</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.228228</td>\n",
       "      <td>0.142643</td>\n",
       "      <td>0.528529</td>\n",
       "      <td>0.802803</td>\n",
       "      <td>0.733734</td>\n",
       "      <td>0.134134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2333</th>\n",
       "      <td>0.986674</td>\n",
       "      <td>0.886398</td>\n",
       "      <td>0.885223</td>\n",
       "      <td>0.222219</td>\n",
       "      <td>0.222219</td>\n",
       "      <td>0.203331</td>\n",
       "      <td>0.239211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.558326</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061562</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.938939</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.228228</td>\n",
       "      <td>0.142643</td>\n",
       "      <td>0.528529</td>\n",
       "      <td>0.802803</td>\n",
       "      <td>0.733734</td>\n",
       "      <td>0.134134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2334 rows Ã— 491 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6    \\\n",
       "0     0.270006  0.616268  0.614537  0.889855  0.889855  0.891098  0.900855   \n",
       "1     0.083130  0.062072  0.061091  0.127694  0.127694  0.122157  0.140623   \n",
       "2     0.736763  0.904569  0.901883  0.942353  0.942353  0.945712  0.949003   \n",
       "3     0.872793  0.926812  0.926584  0.862798  0.862798  0.850648  0.859840   \n",
       "4     0.090079  0.234805  0.232698  0.572573  0.572573  0.621191  0.589874   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2329  0.793802  0.745054  0.742796  0.555385  0.555385  0.537524  0.559688   \n",
       "2330  0.857986  0.760795  0.758566  0.481947  0.481947  0.450607  0.505775   \n",
       "2331  0.964639  0.865542  0.864202  0.345782  0.345782  0.318538  0.368198   \n",
       "2332  0.442278  0.663397  0.661968  0.812899  0.812899  0.802238  0.828281   \n",
       "2333  0.986674  0.886398  0.885223  0.222219  0.222219  0.203331  0.239211   \n",
       "\n",
       "           7         8         9    ...       481       482       483  484  \\\n",
       "0     0.327238  0.119122  0.242768  ...  0.314815  0.314815  0.619119  0.0   \n",
       "1     0.052177  0.078007  0.294970  ...  0.314815  0.314815  0.619119  0.0   \n",
       "2     0.909945  0.706564  0.162829  ...  0.314815  0.314815  0.619119  0.0   \n",
       "3     0.895038  0.788793  0.616153  ...  0.314815  0.314815  0.619119  0.0   \n",
       "4     0.037066  0.022950  0.071108  ...  0.314815  0.314815  0.619119  0.0   \n",
       "...        ...       ...       ...  ...       ...       ...       ...  ...   \n",
       "2329  0.000000  0.000000  0.657090  ...  0.061562  0.000000  0.938939  0.0   \n",
       "2330  0.000000  0.000000  0.529989  ...  0.061562  0.000000  0.938939  0.0   \n",
       "2331  0.610694  0.761628  0.592665  ...  0.061562  0.000000  0.938939  0.0   \n",
       "2332  0.686671  0.507499  0.447593  ...  0.061562  0.000000  0.938939  0.0   \n",
       "2333  0.000000  0.000000  0.558326  ...  0.061562  0.000000  0.938939  0.0   \n",
       "\n",
       "           485       486       487       488       489       490  \n",
       "0     0.946947  0.394394  0.000000  0.666667  0.376877  0.277277  \n",
       "1     0.946947  0.394394  0.000000  0.666667  0.376877  0.277277  \n",
       "2     0.946947  0.394394  0.000000  0.666667  0.376877  0.277277  \n",
       "3     0.946947  0.394394  0.000000  0.666667  0.376877  0.277277  \n",
       "4     0.946947  0.394394  0.000000  0.666667  0.376877  0.277277  \n",
       "...        ...       ...       ...       ...       ...       ...  \n",
       "2329  0.228228  0.142643  0.528529  0.802803  0.733734  0.134134  \n",
       "2330  0.228228  0.142643  0.528529  0.802803  0.733734  0.134134  \n",
       "2331  0.228228  0.142643  0.528529  0.802803  0.733734  0.134134  \n",
       "2332  0.228228  0.142643  0.528529  0.802803  0.733734  0.134134  \n",
       "2333  0.228228  0.142643  0.528529  0.802803  0.733734  0.134134  \n",
       "\n",
       "[2334 rows x 491 columns]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vincent\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:2670: FutureWarning: The default value of `copy` will change from False to True in 0.23 in order to make it more consistent with the default `copy` values of other functions in :mod:`sklearn.preprocessing` and prevent unexpected side effects by modifying the value of `X` inplace. To avoid inplace modifications of `X`, it is recommended to explicitly set `copy=True`\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "val_batch = pd.merge(val_batch,macro_data, on =\"eom\" )\n",
    "val_batch.drop(columns=[\"eom\"], inplace = True)\n",
    "val_batch.drop(columns=delete_cols, inplace = True)\n",
    "val_batch=val_batch.loc[False==val_batch.isna().any(axis=1),:]\n",
    "X_val_batch=quantile_transform(np.array(val_batch.drop(columns=['ret_exc_lead1m'])), n_quantiles=1000)\n",
    "X_val_batch=pd.DataFrame(X_val_batch)\n",
    "Y_val_batch = val_batch[\"ret_exc_lead1m\"].astype(float)\n",
    "\n",
    "# X_val_batch.fillna(X_val_batch.median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_batch=None\n",
    "X_val_batch=None\n",
    "Y_val_batch=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop: 1\n",
      "got data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vincent\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:2670: FutureWarning: The default value of `copy` will change from False to True in 0.23 in order to make it more consistent with the default `copy` values of other functions in :mod:`sklearn.preprocessing` and prevent unexpected side effects by modifying the value of `X` inplace. To avoid inplace modifications of `X`, it is recommended to explicitly set `copy=True`\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "4800/4800 [==============================] - 0s 58us/step - loss: 2287.6189\n",
      "Epoch 2/50\n",
      "4800/4800 [==============================] - 0s 30us/step - loss: 2103.6293\n",
      "Epoch 3/50\n",
      "4800/4800 [==============================] - 0s 30us/step - loss: 4387.7837\n",
      "Epoch 4/50\n",
      "4800/4800 [==============================] - 0s 31us/step - loss: 2071.3343\n",
      "Epoch 5/50\n",
      "4800/4800 [==============================] - 0s 30us/step - loss: 1690.9880\n",
      "Epoch 6/50\n",
      "4800/4800 [==============================] - 0s 31us/step - loss: 1834.6216\n",
      "Epoch 7/50\n",
      "4800/4800 [==============================] - 0s 30us/step - loss: 2102.4191\n",
      "Epoch 8/50\n",
      "4800/4800 [==============================] - 0s 29us/step - loss: 2169.8942\n",
      "Epoch 9/50\n",
      "4800/4800 [==============================] - 0s 29us/step - loss: 645.6185\n",
      "Epoch 10/50\n",
      "4800/4800 [==============================] - 0s 29us/step - loss: 1520.3692\n",
      "Epoch 11/50\n",
      "4800/4800 [==============================] - 0s 30us/step - loss: 1332.0955\n",
      "Epoch 12/50\n",
      "4800/4800 [==============================] - 0s 29us/step - loss: 896.6478\n",
      "Epoch 13/50\n",
      "4800/4800 [==============================] - 0s 30us/step - loss: 598.2712\n",
      "Epoch 14/50\n",
      "4800/4800 [==============================] - 0s 29us/step - loss: 736.2550\n",
      "Epoch 15/50\n",
      "4800/4800 [==============================] - 0s 30us/step - loss: 523.3794\n",
      "Epoch 16/50\n",
      "4800/4800 [==============================] - 0s 29us/step - loss: 1311.0794\n",
      "Epoch 17/50\n",
      "4800/4800 [==============================] - 0s 32us/step - loss: 1333.9598\n",
      "Epoch 18/50\n",
      "4800/4800 [==============================] - 0s 29us/step - loss: 536.4740\n",
      "Epoch 19/50\n",
      "4800/4800 [==============================] - 0s 30us/step - loss: 698.7177\n",
      "Epoch 20/50\n",
      "4800/4800 [==============================] - 0s 30us/step - loss: 313.7886\n",
      "Epoch 21/50\n",
      "4800/4800 [==============================] - 0s 30us/step - loss: 402.6939\n",
      "Epoch 22/50\n",
      "4800/4800 [==============================] - 0s 30us/step - loss: 319.2099\n",
      "Epoch 23/50\n",
      "4800/4800 [==============================] - 0s 31us/step - loss: 507.4465\n",
      "Epoch 24/50\n",
      "4800/4800 [==============================] - 0s 30us/step - loss: 581.9211\n",
      "Epoch 25/50\n",
      "4800/4800 [==============================] - 0s 29us/step - loss: 415.8858\n",
      "Epoch 26/50\n",
      "4800/4800 [==============================] - 0s 30us/step - loss: 341.7710\n",
      "Epoch 27/50\n",
      "4800/4800 [==============================] - 0s 31us/step - loss: 547.9318\n",
      "Epoch 28/50\n",
      "4800/4800 [==============================] - 0s 30us/step - loss: 483.7532\n",
      "Epoch 29/50\n",
      "4800/4800 [==============================] - 0s 31us/step - loss: 480.0842\n",
      "Epoch 30/50\n",
      "4800/4800 [==============================] - 0s 33us/step - loss: 334.2771\n",
      "Epoch 31/50\n",
      "4800/4800 [==============================] - 0s 30us/step - loss: 232.7310\n",
      "Epoch 32/50\n",
      "4800/4800 [==============================] - 0s 30us/step - loss: 382.3926\n",
      "Epoch 33/50\n",
      "4800/4800 [==============================] - 0s 31us/step - loss: 441.9770\n",
      "Epoch 34/50\n",
      "4800/4800 [==============================] - 0s 31us/step - loss: 424.0545\n",
      "Epoch 35/50\n",
      "4800/4800 [==============================] - 0s 30us/step - loss: 222.7203\n",
      "Epoch 36/50\n",
      "4800/4800 [==============================] - 0s 30us/step - loss: 171.1545\n",
      "Epoch 37/50\n",
      "4800/4800 [==============================] - 0s 31us/step - loss: 230.1121\n",
      "Epoch 38/50\n",
      "4800/4800 [==============================] - 0s 31us/step - loss: 400.7023\n",
      "Epoch 39/50\n",
      "4800/4800 [==============================] - 0s 30us/step - loss: 246.0117\n",
      "Epoch 40/50\n",
      "4800/4800 [==============================] - 0s 31us/step - loss: 383.8677\n",
      "Epoch 41/50\n",
      "4800/4800 [==============================] - 0s 31us/step - loss: 123.2625\n",
      "Epoch 42/50\n",
      "4800/4800 [==============================] - 0s 30us/step - loss: 165.8838\n",
      "Epoch 43/50\n",
      "4800/4800 [==============================] - 0s 30us/step - loss: 148.9555\n",
      "Epoch 44/50\n",
      "4800/4800 [==============================] - 0s 30us/step - loss: 147.2723\n",
      "Epoch 45/50\n",
      "4800/4800 [==============================] - 0s 30us/step - loss: 283.9809\n",
      "Epoch 46/50\n",
      "4800/4800 [==============================] - 0s 31us/step - loss: 227.6550\n",
      "Epoch 47/50\n",
      "4800/4800 [==============================] - 0s 31us/step - loss: 134.4345\n",
      "Epoch 48/50\n",
      "4800/4800 [==============================] - 0s 30us/step - loss: 145.8335\n",
      "Epoch 49/50\n",
      "4800/4800 [==============================] - 0s 29us/step - loss: 243.8472\n",
      "Epoch 50/50\n",
      "4800/4800 [==============================] - 0s 30us/step - loss: 107.9442\n",
      "Epoch 1/50\n",
      "4800/4800 [==============================] - 0s 50us/step - loss: -269.0857\n",
      "Epoch 2/50\n",
      "4800/4800 [==============================] - 0s 32us/step - loss: -167.7766\n",
      "Epoch 3/50\n",
      "4800/4800 [==============================] - 0s 33us/step - loss: -257.2431\n",
      "Epoch 4/50\n",
      "4800/4800 [==============================] - 0s 32us/step - loss: -214.3502\n",
      "Epoch 5/50\n",
      "4800/4800 [==============================] - 0s 33us/step - loss: -301.1228\n",
      "Epoch 6/50\n",
      "4800/4800 [==============================] - 0s 34us/step - loss: -103.3306\n",
      "Epoch 7/50\n",
      "4800/4800 [==============================] - 0s 34us/step - loss: -172.8536\n",
      "Epoch 8/50\n",
      "4800/4800 [==============================] - 0s 32us/step - loss: -238.1367\n",
      "Epoch 9/50\n",
      "4800/4800 [==============================] - 0s 34us/step - loss: -291.0984\n",
      "Epoch 10/50\n",
      "4800/4800 [==============================] - 0s 34us/step - loss: -205.2218\n",
      "Epoch 11/50\n",
      "4800/4800 [==============================] - 0s 32us/step - loss: -195.1925\n",
      "Epoch 12/50\n",
      "4800/4800 [==============================] - 0s 36us/step - loss: -118.7154\n",
      "Epoch 13/50\n",
      "4800/4800 [==============================] - 0s 34us/step - loss: -91.8734\n",
      "Epoch 14/50\n",
      "4800/4800 [==============================] - 0s 34us/step - loss: -243.0454\n",
      "Epoch 15/50\n",
      "4800/4800 [==============================] - 0s 32us/step - loss: -157.2576\n",
      "Epoch 16/50\n",
      "4800/4800 [==============================] - 0s 32us/step - loss: -284.2328\n",
      "Epoch 17/50\n",
      "4800/4800 [==============================] - 0s 32us/step - loss: -189.1414\n",
      "Epoch 18/50\n",
      "4800/4800 [==============================] - 0s 33us/step - loss: -71.6192\n",
      "Epoch 19/50\n",
      "4800/4800 [==============================] - 0s 32us/step - loss: -134.5730\n",
      "Epoch 20/50\n",
      "4800/4800 [==============================] - 0s 33us/step - loss: -156.5656\n",
      "Epoch 21/50\n",
      "4800/4800 [==============================] - 0s 32us/step - loss: -233.7347\n",
      "Epoch 22/50\n",
      "4800/4800 [==============================] - 0s 35us/step - loss: -151.6092\n",
      "Epoch 23/50\n",
      "4800/4800 [==============================] - 0s 32us/step - loss: -197.2135\n",
      "Epoch 24/50\n",
      "4800/4800 [==============================] - 0s 33us/step - loss: -181.8428\n",
      "Epoch 25/50\n",
      "4800/4800 [==============================] - 0s 35us/step - loss: -197.8652\n",
      "Epoch 26/50\n",
      "4800/4800 [==============================] - 0s 34us/step - loss: -193.4075\n",
      "Epoch 27/50\n",
      "4800/4800 [==============================] - 0s 32us/step - loss: -353.4065\n",
      "Epoch 28/50\n",
      "4800/4800 [==============================] - 0s 33us/step - loss: -186.8779\n",
      "Epoch 29/50\n",
      "4800/4800 [==============================] - 0s 33us/step - loss: -185.4398\n",
      "Epoch 30/50\n",
      "4800/4800 [==============================] - 0s 35us/step - loss: -167.0877\n",
      "Epoch 31/50\n",
      "4800/4800 [==============================] - 0s 32us/step - loss: -144.4859\n",
      "Epoch 32/50\n",
      "4800/4800 [==============================] - 0s 33us/step - loss: -181.6105\n",
      "Epoch 33/50\n",
      "4800/4800 [==============================] - 0s 32us/step - loss: -220.6658\n",
      "Epoch 34/50\n",
      "4800/4800 [==============================] - 0s 32us/step - loss: -138.6248\n",
      "Epoch 35/50\n",
      "4800/4800 [==============================] - 0s 33us/step - loss: -329.6391\n",
      "Epoch 36/50\n",
      "4800/4800 [==============================] - 0s 35us/step - loss: -175.8882\n",
      "Epoch 37/50\n",
      "4800/4800 [==============================] - 0s 38us/step - loss: -366.5752\n",
      "Epoch 38/50\n",
      "4800/4800 [==============================] - 0s 38us/step - loss: -198.5343\n",
      "Epoch 39/50\n",
      "4800/4800 [==============================] - 0s 36us/step - loss: -200.6124\n",
      "Epoch 40/50\n",
      "4800/4800 [==============================] - 0s 36us/step - loss: -395.1888\n",
      "Epoch 41/50\n",
      "4800/4800 [==============================] - 0s 37us/step - loss: -266.5114\n",
      "Epoch 42/50\n",
      "4800/4800 [==============================] - 0s 36us/step - loss: -217.9800\n",
      "Epoch 43/50\n",
      "4800/4800 [==============================] - 0s 36us/step - loss: -120.6677\n",
      "Epoch 44/50\n",
      "4800/4800 [==============================] - 0s 36us/step - loss: -271.8896\n",
      "Epoch 45/50\n",
      "4800/4800 [==============================] - 0s 35us/step - loss: -162.4326\n",
      "Epoch 46/50\n",
      "4800/4800 [==============================] - 0s 35us/step - loss: -157.0579\n",
      "Epoch 47/50\n",
      "4800/4800 [==============================] - 0s 35us/step - loss: -214.9421\n",
      "Epoch 48/50\n",
      "4800/4800 [==============================] - 0s 34us/step - loss: -215.8887\n",
      "Epoch 49/50\n",
      "4800/4800 [==============================] - 0s 34us/step - loss: -229.1940\n",
      "Epoch 50/50\n",
      "4800/4800 [==============================] - 0s 33us/step - loss: -291.4954\n",
      "Epoch 1/50\n",
      "4800/4800 [==============================] - 0s 54us/step - loss: 313.5288\n",
      "Epoch 2/50\n",
      "4800/4800 [==============================] - 0s 30us/step - loss: 365.5974\n",
      "Epoch 3/50\n",
      "4800/4800 [==============================] - 0s 29us/step - loss: 135.9283\n",
      "Epoch 4/50\n",
      "4800/4800 [==============================] - 0s 31us/step - loss: 104.6826\n",
      "Epoch 5/50\n",
      "4800/4800 [==============================] - 0s 31us/step - loss: 105.5756\n",
      "Epoch 6/50\n",
      "4800/4800 [==============================] - 0s 31us/step - loss: 196.3633\n",
      "Epoch 7/50\n",
      "4800/4800 [==============================] - 0s 30us/step - loss: 131.2670\n",
      "Epoch 8/50\n",
      "4800/4800 [==============================] - 0s 28us/step - loss: 215.8065\n",
      "Epoch 9/50\n",
      "4800/4800 [==============================] - 0s 31us/step - loss: 183.8535\n",
      "Epoch 10/50\n",
      "4800/4800 [==============================] - 0s 33us/step - loss: 137.9955\n",
      "Epoch 11/50\n",
      "4800/4800 [==============================] - 0s 32us/step - loss: 101.3727\n",
      "Epoch 12/50\n",
      "4800/4800 [==============================] - 0s 32us/step - loss: 60.0223\n",
      "Epoch 13/50\n",
      "4800/4800 [==============================] - 0s 32us/step - loss: 134.6051\n",
      "Epoch 14/50\n",
      "4800/4800 [==============================] - 0s 33us/step - loss: 195.7466\n",
      "Epoch 15/50\n",
      "4800/4800 [==============================] - 0s 30us/step - loss: 165.2712\n",
      "Epoch 16/50\n",
      "4800/4800 [==============================] - 0s 30us/step - loss: 38.8954\n",
      "Epoch 17/50\n",
      "4800/4800 [==============================] - 0s 28us/step - loss: 30.5696\n",
      "Epoch 18/50\n",
      "4800/4800 [==============================] - 0s 30us/step - loss: 150.2450\n",
      "Epoch 19/50\n",
      "4800/4800 [==============================] - 0s 29us/step - loss: 49.1475\n",
      "Epoch 20/50\n",
      "4800/4800 [==============================] - 0s 29us/step - loss: 43.0436\n",
      "Epoch 21/50\n",
      "4800/4800 [==============================] - 0s 29us/step - loss: 78.1322\n",
      "Epoch 22/50\n",
      "4800/4800 [==============================] - 0s 29us/step - loss: 61.9222\n",
      "Epoch 23/50\n",
      "4800/4800 [==============================] - 0s 29us/step - loss: 40.4702\n",
      "Epoch 24/50\n",
      "4800/4800 [==============================] - 0s 29us/step - loss: 86.2694\n",
      "Epoch 25/50\n",
      "4800/4800 [==============================] - 0s 29us/step - loss: 79.7666\n",
      "Epoch 26/50\n",
      "4800/4800 [==============================] - 0s 29us/step - loss: 126.9932\n",
      "Epoch 27/50\n",
      "4800/4800 [==============================] - 0s 31us/step - loss: 16.2388\n",
      "Epoch 28/50\n",
      "4800/4800 [==============================] - 0s 31us/step - loss: 50.3780\n",
      "Epoch 29/50\n",
      "4800/4800 [==============================] - 0s 30us/step - loss: 42.8021\n",
      "Epoch 30/50\n",
      "4800/4800 [==============================] - 0s 30us/step - loss: 26.7158\n",
      "Epoch 31/50\n",
      "4800/4800 [==============================] - 0s 30us/step - loss: 22.2723\n",
      "Epoch 32/50\n",
      "4800/4800 [==============================] - 0s 30us/step - loss: 64.4367\n",
      "Epoch 33/50\n",
      "4800/4800 [==============================] - 0s 29us/step - loss: 31.1134\n",
      "Epoch 34/50\n",
      "4800/4800 [==============================] - 0s 29us/step - loss: 41.2884\n",
      "Epoch 35/50\n",
      "4800/4800 [==============================] - 0s 30us/step - loss: 44.0865\n",
      "Epoch 36/50\n",
      "4800/4800 [==============================] - 0s 32us/step - loss: 33.4267\n",
      "Epoch 37/50\n",
      "4800/4800 [==============================] - 0s 25us/step - loss: 37.9160\n",
      "Epoch 38/50\n",
      "4800/4800 [==============================] - 0s 29us/step - loss: 35.1523\n",
      "Epoch 39/50\n",
      "4800/4800 [==============================] - 0s 34us/step - loss: 22.5210\n",
      "Epoch 40/50\n",
      "4800/4800 [==============================] - 0s 29us/step - loss: 29.5502\n",
      "Epoch 41/50\n",
      "4800/4800 [==============================] - 0s 31us/step - loss: 27.4824\n",
      "Epoch 42/50\n",
      "4800/4800 [==============================] - 0s 31us/step - loss: 24.0990\n",
      "Epoch 43/50\n",
      "4800/4800 [==============================] - 0s 34us/step - loss: 30.4991\n",
      "Epoch 44/50\n",
      "4800/4800 [==============================] - 0s 32us/step - loss: 27.1840\n",
      "Epoch 45/50\n",
      "4800/4800 [==============================] - 0s 33us/step - loss: 24.9984\n",
      "Epoch 46/50\n",
      "4800/4800 [==============================] - 0s 34us/step - loss: 11.5329\n",
      "Epoch 47/50\n",
      "4800/4800 [==============================] - 0s 33us/step - loss: 21.0309\n",
      "Epoch 48/50\n",
      "4800/4800 [==============================] - 0s 34us/step - loss: 37.8020\n",
      "Epoch 49/50\n",
      "4800/4800 [==============================] - 0s 34us/step - loss: 20.1742\n",
      "Epoch 50/50\n",
      "4800/4800 [==============================] - 0s 30us/step - loss: 18.4132\n",
      "Loop: 2\n",
      "got data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vincent\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:2670: FutureWarning: The default value of `copy` will change from False to True in 0.23 in order to make it more consistent with the default `copy` values of other functions in :mod:`sklearn.preprocessing` and prevent unexpected side effects by modifying the value of `X` inplace. To avoid inplace modifications of `X`, it is recommended to explicitly set `copy=True`\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "4621/4621 [==============================] - 0s 56us/step - loss: 25.5092\n",
      "Epoch 2/50\n",
      "4621/4621 [==============================] - 0s 32us/step - loss: 8.5109\n",
      "Epoch 3/50\n",
      "4621/4621 [==============================] - 0s 31us/step - loss: 11.8602\n",
      "Epoch 4/50\n",
      "4621/4621 [==============================] - 0s 31us/step - loss: 9.1795\n",
      "Epoch 5/50\n",
      "4621/4621 [==============================] - 0s 33us/step - loss: 12.3103\n",
      "Epoch 6/50\n",
      "4621/4621 [==============================] - 0s 31us/step - loss: 11.6379\n",
      "Epoch 7/50\n",
      "4621/4621 [==============================] - 0s 30us/step - loss: 14.8050\n",
      "Epoch 8/50\n",
      "4621/4621 [==============================] - 0s 33us/step - loss: 9.4632\n",
      "Epoch 9/50\n",
      "4621/4621 [==============================] - 0s 32us/step - loss: 7.6149\n",
      "Epoch 10/50\n",
      "4621/4621 [==============================] - 0s 31us/step - loss: 7.0847\n",
      "Epoch 11/50\n",
      "4621/4621 [==============================] - 0s 32us/step - loss: 8.8178\n",
      "Epoch 12/50\n",
      "4621/4621 [==============================] - 0s 32us/step - loss: 9.2318\n",
      "Epoch 13/50\n",
      "4621/4621 [==============================] - 0s 30us/step - loss: 10.6414\n",
      "Epoch 14/50\n",
      "4621/4621 [==============================] - 0s 29us/step - loss: 7.1481\n",
      "Epoch 15/50\n",
      "4621/4621 [==============================] - 0s 32us/step - loss: 8.2577\n",
      "Epoch 16/50\n",
      "4621/4621 [==============================] - 0s 32us/step - loss: 6.1415\n",
      "Epoch 17/50\n",
      "4621/4621 [==============================] - 0s 32us/step - loss: 4.2842\n",
      "Epoch 18/50\n",
      "4621/4621 [==============================] - 0s 31us/step - loss: 13.8896\n",
      "Epoch 19/50\n",
      "4621/4621 [==============================] - 0s 32us/step - loss: 2.4509\n",
      "Epoch 20/50\n",
      "4621/4621 [==============================] - 0s 32us/step - loss: 6.0504\n",
      "Epoch 21/50\n",
      "4621/4621 [==============================] - 0s 30us/step - loss: 4.7125\n",
      "Epoch 22/50\n",
      "4621/4621 [==============================] - 0s 30us/step - loss: 2.9644\n",
      "Epoch 23/50\n",
      "4621/4621 [==============================] - 0s 30us/step - loss: 7.2610\n",
      "Epoch 24/50\n",
      "4621/4621 [==============================] - 0s 32us/step - loss: 4.4770\n",
      "Epoch 25/50\n",
      "4621/4621 [==============================] - 0s 31us/step - loss: 4.2377\n",
      "Epoch 26/50\n",
      "4621/4621 [==============================] - 0s 31us/step - loss: 3.0765\n",
      "Epoch 27/50\n",
      "4621/4621 [==============================] - 0s 32us/step - loss: 10.0150\n",
      "Epoch 28/50\n",
      "4621/4621 [==============================] - 0s 30us/step - loss: 2.0596\n",
      "Epoch 29/50\n",
      "4621/4621 [==============================] - 0s 31us/step - loss: 3.0054\n",
      "Epoch 30/50\n",
      "4621/4621 [==============================] - 0s 31us/step - loss: 2.5974\n",
      "Epoch 31/50\n",
      "4621/4621 [==============================] - 0s 30us/step - loss: 3.9489\n",
      "Epoch 32/50\n",
      "4621/4621 [==============================] - 0s 30us/step - loss: 1.5141\n",
      "Epoch 33/50\n",
      "4621/4621 [==============================] - 0s 33us/step - loss: 2.0108\n",
      "Epoch 34/50\n",
      "4621/4621 [==============================] - 0s 32us/step - loss: 2.3705\n",
      "Epoch 35/50\n",
      "4621/4621 [==============================] - 0s 31us/step - loss: 2.2295\n",
      "Epoch 36/50\n",
      "4621/4621 [==============================] - 0s 31us/step - loss: 2.9155\n",
      "Epoch 37/50\n",
      "4621/4621 [==============================] - 0s 31us/step - loss: 4.0261\n",
      "Epoch 38/50\n",
      "4621/4621 [==============================] - 0s 31us/step - loss: 1.2846\n",
      "Epoch 39/50\n",
      "4621/4621 [==============================] - 0s 31us/step - loss: 2.2357\n",
      "Epoch 40/50\n",
      "4621/4621 [==============================] - 0s 31us/step - loss: 1.9946\n",
      "Epoch 41/50\n",
      "4621/4621 [==============================] - 0s 30us/step - loss: 2.5334\n",
      "Epoch 42/50\n",
      "4621/4621 [==============================] - 0s 30us/step - loss: 5.1006\n",
      "Epoch 43/50\n",
      "4621/4621 [==============================] - 0s 30us/step - loss: 0.8874\n",
      "Epoch 44/50\n",
      "4621/4621 [==============================] - 0s 32us/step - loss: 1.7172\n",
      "Epoch 45/50\n",
      "4621/4621 [==============================] - 0s 32us/step - loss: 0.9364\n",
      "Epoch 46/50\n",
      "4621/4621 [==============================] - 0s 32us/step - loss: 1.8521\n",
      "Epoch 47/50\n",
      "4621/4621 [==============================] - 0s 31us/step - loss: 1.7000\n",
      "Epoch 48/50\n",
      "4621/4621 [==============================] - 0s 31us/step - loss: 1.4377\n",
      "Epoch 49/50\n",
      "4621/4621 [==============================] - 0s 32us/step - loss: 3.0505\n",
      "Epoch 50/50\n",
      "4621/4621 [==============================] - 0s 33us/step - loss: 3.7961\n",
      "Epoch 1/50\n",
      "4621/4621 [==============================] - 0s 53us/step - loss: -0.6017\n",
      "Epoch 2/50\n",
      "4621/4621 [==============================] - 0s 34us/step - loss: -0.3917\n",
      "Epoch 3/50\n",
      "4621/4621 [==============================] - 0s 32us/step - loss: -0.3233\n",
      "Epoch 4/50\n",
      "4621/4621 [==============================] - 0s 34us/step - loss: -0.1715\n",
      "Epoch 5/50\n",
      "4621/4621 [==============================] - 0s 33us/step - loss: -0.2717\n",
      "Epoch 6/50\n",
      "4621/4621 [==============================] - 0s 33us/step - loss: -0.2895\n",
      "Epoch 7/50\n",
      "4621/4621 [==============================] - 0s 34us/step - loss: -0.2014\n",
      "Epoch 8/50\n",
      "4621/4621 [==============================] - 0s 32us/step - loss: -0.9265\n",
      "Epoch 9/50\n",
      "4621/4621 [==============================] - 0s 32us/step - loss: -0.5572\n",
      "Epoch 10/50\n",
      "4621/4621 [==============================] - 0s 33us/step - loss: -0.5737\n",
      "Epoch 11/50\n",
      "4621/4621 [==============================] - 0s 32us/step - loss: -0.3333\n",
      "Epoch 12/50\n",
      "4621/4621 [==============================] - 0s 34us/step - loss: -0.2836\n",
      "Epoch 13/50\n",
      "4621/4621 [==============================] - 0s 34us/step - loss: -0.1021\n",
      "Epoch 14/50\n",
      "4621/4621 [==============================] - 0s 32us/step - loss: -0.1970\n",
      "Epoch 15/50\n",
      "4621/4621 [==============================] - 0s 34us/step - loss: -0.3255\n",
      "Epoch 16/50\n",
      "4621/4621 [==============================] - 0s 34us/step - loss: -0.5895\n",
      "Epoch 17/50\n",
      "4621/4621 [==============================] - 0s 33us/step - loss: -0.4083\n",
      "Epoch 18/50\n",
      "4621/4621 [==============================] - 0s 33us/step - loss: -0.8765\n",
      "Epoch 19/50\n",
      "4621/4621 [==============================] - 0s 34us/step - loss: -0.2364\n",
      "Epoch 20/50\n",
      "4621/4621 [==============================] - 0s 32us/step - loss: -0.1915\n",
      "Epoch 21/50\n",
      "4621/4621 [==============================] - 0s 33us/step - loss: -0.3524\n",
      "Epoch 22/50\n",
      "4621/4621 [==============================] - 0s 33us/step - loss: -0.2885\n",
      "Epoch 23/50\n",
      "4621/4621 [==============================] - 0s 34us/step - loss: -0.5367\n",
      "Epoch 24/50\n",
      "4621/4621 [==============================] - 0s 33us/step - loss: -0.2644\n",
      "Epoch 25/50\n",
      "4621/4621 [==============================] - 0s 34us/step - loss: -0.2548\n",
      "Epoch 26/50\n",
      "4621/4621 [==============================] - 0s 32us/step - loss: -0.5666\n",
      "Epoch 27/50\n",
      "4621/4621 [==============================] - 0s 33us/step - loss: -0.2889\n",
      "Epoch 28/50\n",
      "4621/4621 [==============================] - 0s 34us/step - loss: -0.1763\n",
      "Epoch 29/50\n",
      "4621/4621 [==============================] - 0s 34us/step - loss: -0.2987\n",
      "Epoch 30/50\n",
      "4621/4621 [==============================] - 0s 33us/step - loss: -0.3404\n",
      "Epoch 31/50\n",
      "4621/4621 [==============================] - 0s 35us/step - loss: -0.2293\n",
      "Epoch 32/50\n",
      "4621/4621 [==============================] - 0s 34us/step - loss: -0.2497\n",
      "Epoch 33/50\n",
      "4621/4621 [==============================] - 0s 36us/step - loss: -0.6251\n",
      "Epoch 34/50\n",
      "4621/4621 [==============================] - 0s 34us/step - loss: -0.2615\n",
      "Epoch 35/50\n",
      "4621/4621 [==============================] - 0s 32us/step - loss: -0.5073\n",
      "Epoch 36/50\n",
      "4621/4621 [==============================] - 0s 33us/step - loss: -0.2365\n",
      "Epoch 37/50\n",
      "4621/4621 [==============================] - 0s 33us/step - loss: -0.7674\n",
      "Epoch 38/50\n",
      "4621/4621 [==============================] - 0s 33us/step - loss: -0.3234\n",
      "Epoch 39/50\n",
      "4621/4621 [==============================] - 0s 34us/step - loss: -0.2264\n",
      "Epoch 40/50\n",
      "4621/4621 [==============================] - 0s 33us/step - loss: -0.3669\n",
      "Epoch 41/50\n",
      "4621/4621 [==============================] - 0s 33us/step - loss: -0.3720\n",
      "Epoch 42/50\n",
      "4621/4621 [==============================] - 0s 34us/step - loss: -0.2104\n",
      "Epoch 43/50\n",
      "4621/4621 [==============================] - 0s 33us/step - loss: -0.3799\n",
      "Epoch 44/50\n",
      "4621/4621 [==============================] - 0s 34us/step - loss: -0.5088\n",
      "Epoch 45/50\n",
      "4621/4621 [==============================] - 0s 34us/step - loss: -0.5642\n",
      "Epoch 46/50\n",
      "4621/4621 [==============================] - 0s 32us/step - loss: -0.3223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50\n",
      "4621/4621 [==============================] - 0s 33us/step - loss: -0.4173\n",
      "Epoch 48/50\n",
      "4621/4621 [==============================] - 0s 38us/step - loss: -0.1808\n",
      "Epoch 49/50\n",
      "4621/4621 [==============================] - 0s 32us/step - loss: -0.2374\n",
      "Epoch 50/50\n",
      "4621/4621 [==============================] - 0s 32us/step - loss: -0.3845\n",
      "Epoch 1/50\n",
      "4621/4621 [==============================] - 0s 56us/step - loss: 0.9488\n",
      "Epoch 2/50\n",
      "4621/4621 [==============================] - 0s 30us/step - loss: 0.6382\n",
      "Epoch 3/50\n",
      "4621/4621 [==============================] - 0s 30us/step - loss: 0.5585\n",
      "Epoch 4/50\n",
      "4621/4621 [==============================] - 0s 30us/step - loss: 0.5351\n",
      "Epoch 5/50\n",
      "4621/4621 [==============================] - 0s 30us/step - loss: 0.1880\n",
      "Epoch 6/50\n",
      "4621/4621 [==============================] - 0s 30us/step - loss: 0.2844\n",
      "Epoch 7/50\n",
      "4621/4621 [==============================] - 0s 30us/step - loss: 0.3994\n",
      "Epoch 8/50\n",
      "4621/4621 [==============================] - 0s 30us/step - loss: 0.3625\n",
      "Epoch 9/50\n",
      "4621/4621 [==============================] - 0s 31us/step - loss: 0.2450\n",
      "Epoch 10/50\n",
      "4621/4621 [==============================] - 0s 31us/step - loss: 0.5445\n",
      "Epoch 11/50\n",
      "4621/4621 [==============================] - 0s 30us/step - loss: 0.1475\n",
      "Epoch 12/50\n",
      "4621/4621 [==============================] - 0s 31us/step - loss: 0.2067\n",
      "Epoch 13/50\n",
      "4621/4621 [==============================] - 0s 32us/step - loss: 0.1286\n",
      "Epoch 14/50\n",
      "4621/4621 [==============================] - 0s 32us/step - loss: 0.2378\n",
      "Epoch 15/50\n",
      "4621/4621 [==============================] - 0s 31us/step - loss: 0.1522\n",
      "Epoch 16/50\n",
      "4621/4621 [==============================] - 0s 31us/step - loss: 0.2736\n",
      "Epoch 17/50\n",
      "4621/4621 [==============================] - 0s 31us/step - loss: 0.1325\n",
      "Epoch 18/50\n",
      "4621/4621 [==============================] - 0s 31us/step - loss: 0.1222\n",
      "Epoch 19/50\n",
      "4621/4621 [==============================] - 0s 32us/step - loss: 0.1458\n",
      "Epoch 20/50\n",
      "4621/4621 [==============================] - 0s 31us/step - loss: 0.2004\n",
      "Epoch 21/50\n",
      "4621/4621 [==============================] - 0s 32us/step - loss: 0.2325\n",
      "Epoch 22/50\n",
      "4621/4621 [==============================] - 0s 30us/step - loss: 0.1135\n",
      "Epoch 23/50\n",
      "4621/4621 [==============================] - 0s 32us/step - loss: 0.1969\n",
      "Epoch 24/50\n",
      "4621/4621 [==============================] - 0s 32us/step - loss: 0.1699\n",
      "Epoch 25/50\n",
      "4621/4621 [==============================] - 0s 32us/step - loss: 0.1732\n",
      "Epoch 26/50\n",
      "4621/4621 [==============================] - 0s 34us/step - loss: 0.1425\n",
      "Epoch 27/50\n",
      "4621/4621 [==============================] - 0s 31us/step - loss: 0.1063\n",
      "Epoch 28/50\n",
      "4621/4621 [==============================] - 0s 32us/step - loss: 0.1082\n",
      "Epoch 29/50\n",
      "4621/4621 [==============================] - 0s 31us/step - loss: 0.2318\n",
      "Epoch 30/50\n",
      "4621/4621 [==============================] - 0s 33us/step - loss: 0.0710\n",
      "Epoch 31/50\n",
      "4621/4621 [==============================] - 0s 33us/step - loss: 0.1626\n",
      "Epoch 32/50\n",
      "4621/4621 [==============================] - 0s 32us/step - loss: 0.1334\n",
      "Epoch 33/50\n",
      "4621/4621 [==============================] - 0s 34us/step - loss: 0.0794\n",
      "Epoch 34/50\n",
      "4621/4621 [==============================] - 0s 32us/step - loss: 0.0473\n",
      "Epoch 35/50\n",
      "4621/4621 [==============================] - 0s 31us/step - loss: 0.0660\n",
      "Epoch 36/50\n",
      "4621/4621 [==============================] - 0s 32us/step - loss: 0.0295\n",
      "Epoch 37/50\n",
      "4621/4621 [==============================] - 0s 32us/step - loss: 0.0487\n",
      "Epoch 38/50\n",
      "4621/4621 [==============================] - 0s 35us/step - loss: 0.0347\n",
      "Epoch 39/50\n",
      "4621/4621 [==============================] - 0s 40us/step - loss: 0.0822\n",
      "Epoch 40/50\n",
      "4621/4621 [==============================] - 0s 33us/step - loss: 0.0459\n",
      "Epoch 41/50\n",
      "4621/4621 [==============================] - 0s 33us/step - loss: 0.0785\n",
      "Epoch 42/50\n",
      "4621/4621 [==============================] - 0s 32us/step - loss: 0.0542\n",
      "Epoch 43/50\n",
      "4621/4621 [==============================] - 0s 33us/step - loss: 0.0491\n",
      "Epoch 44/50\n",
      "4621/4621 [==============================] - 0s 33us/step - loss: 0.0689\n",
      "Epoch 45/50\n",
      "4621/4621 [==============================] - 0s 32us/step - loss: 0.0270\n",
      "Epoch 46/50\n",
      "4621/4621 [==============================] - 0s 33us/step - loss: 0.0311\n",
      "Epoch 47/50\n",
      "4621/4621 [==============================] - 0s 32us/step - loss: 0.0356\n",
      "Epoch 48/50\n",
      "4621/4621 [==============================] - 0s 32us/step - loss: 0.0656\n",
      "Epoch 49/50\n",
      "4621/4621 [==============================] - 0s 33us/step - loss: 0.0870\n",
      "Epoch 50/50\n",
      "4621/4621 [==============================] - 0s 31us/step - loss: 0.0748\n",
      "Loop: 3\n",
      "got data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vincent\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:2670: FutureWarning: The default value of `copy` will change from False to True in 0.23 in order to make it more consistent with the default `copy` values of other functions in :mod:`sklearn.preprocessing` and prevent unexpected side effects by modifying the value of `X` inplace. To avoid inplace modifications of `X`, it is recommended to explicitly set `copy=True`\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "4610/4610 [==============================] - 0s 57us/step - loss: 0.0234\n",
      "Epoch 2/50\n",
      "4610/4610 [==============================] - 0s 30us/step - loss: 0.0194\n",
      "Epoch 3/50\n",
      "4610/4610 [==============================] - 0s 31us/step - loss: 0.0175\n",
      "Epoch 4/50\n",
      "4610/4610 [==============================] - 0s 29us/step - loss: 0.0157\n",
      "Epoch 5/50\n",
      "4610/4610 [==============================] - 0s 30us/step - loss: 0.0117\n",
      "Epoch 6/50\n",
      "4610/4610 [==============================] - 0s 30us/step - loss: 0.0216\n",
      "Epoch 7/50\n",
      "4610/4610 [==============================] - 0s 30us/step - loss: 0.0167\n",
      "Epoch 8/50\n",
      "4610/4610 [==============================] - 0s 30us/step - loss: 0.0193\n",
      "Epoch 9/50\n",
      "4610/4610 [==============================] - 0s 32us/step - loss: 0.0213\n",
      "Epoch 10/50\n",
      "4610/4610 [==============================] - 0s 30us/step - loss: 0.0089\n",
      "Epoch 11/50\n",
      "4610/4610 [==============================] - 0s 30us/step - loss: 0.0155\n",
      "Epoch 12/50\n",
      "4610/4610 [==============================] - 0s 30us/step - loss: 0.0150\n",
      "Epoch 13/50\n",
      "4610/4610 [==============================] - 0s 30us/step - loss: 0.0105\n",
      "Epoch 14/50\n",
      "4610/4610 [==============================] - 0s 30us/step - loss: 0.0100\n",
      "Epoch 15/50\n",
      "4610/4610 [==============================] - 0s 31us/step - loss: 0.0139\n",
      "Epoch 16/50\n",
      "4610/4610 [==============================] - 0s 31us/step - loss: 0.0171\n",
      "Epoch 17/50\n",
      "4610/4610 [==============================] - 0s 30us/step - loss: 0.0152\n",
      "Epoch 18/50\n",
      "4610/4610 [==============================] - 0s 30us/step - loss: 0.0128\n",
      "Epoch 19/50\n",
      "4610/4610 [==============================] - 0s 30us/step - loss: 0.0136\n",
      "Epoch 20/50\n",
      "4610/4610 [==============================] - 0s 30us/step - loss: 0.0158\n",
      "Epoch 21/50\n",
      "4610/4610 [==============================] - 0s 30us/step - loss: 0.0152\n",
      "Epoch 22/50\n",
      "4610/4610 [==============================] - 0s 30us/step - loss: 0.0120\n",
      "Epoch 23/50\n",
      "4610/4610 [==============================] - 0s 30us/step - loss: 0.0137\n",
      "Epoch 24/50\n",
      "4610/4610 [==============================] - 0s 30us/step - loss: 0.0161\n",
      "Epoch 25/50\n",
      "4610/4610 [==============================] - 0s 31us/step - loss: 0.0194\n",
      "Epoch 26/50\n",
      "4610/4610 [==============================] - 0s 31us/step - loss: 0.0166\n",
      "Epoch 27/50\n",
      "4610/4610 [==============================] - 0s 30us/step - loss: 0.0115\n",
      "Epoch 28/50\n",
      "4610/4610 [==============================] - 0s 31us/step - loss: 0.0123\n",
      "Epoch 29/50\n",
      "4610/4610 [==============================] - 0s 30us/step - loss: 0.0137\n",
      "Epoch 30/50\n",
      "4610/4610 [==============================] - 0s 31us/step - loss: 0.0126\n",
      "Epoch 31/50\n",
      "4610/4610 [==============================] - 0s 31us/step - loss: 0.0110\n",
      "Epoch 32/50\n",
      "4610/4610 [==============================] - 0s 30us/step - loss: 0.0123\n",
      "Epoch 33/50\n",
      "4610/4610 [==============================] - 0s 30us/step - loss: 0.0103\n",
      "Epoch 34/50\n",
      "4610/4610 [==============================] - 0s 30us/step - loss: 0.0123\n",
      "Epoch 35/50\n",
      "4610/4610 [==============================] - 0s 30us/step - loss: 0.0141\n",
      "Epoch 36/50\n",
      "4610/4610 [==============================] - 0s 30us/step - loss: 0.0168\n",
      "Epoch 37/50\n",
      "4610/4610 [==============================] - 0s 32us/step - loss: 0.0194\n",
      "Epoch 38/50\n",
      "4610/4610 [==============================] - 0s 30us/step - loss: 0.0124\n",
      "Epoch 39/50\n",
      "4610/4610 [==============================] - 0s 31us/step - loss: 0.0108\n",
      "Epoch 40/50\n",
      "4610/4610 [==============================] - 0s 33us/step - loss: 0.0114\n",
      "Epoch 41/50\n",
      "4610/4610 [==============================] - 0s 31us/step - loss: 0.0131\n",
      "Epoch 42/50\n",
      "4610/4610 [==============================] - 0s 30us/step - loss: 0.0105\n",
      "Epoch 43/50\n",
      "4610/4610 [==============================] - 0s 31us/step - loss: 0.0103\n",
      "Epoch 44/50\n",
      "4610/4610 [==============================] - 0s 31us/step - loss: 0.0175\n",
      "Epoch 45/50\n",
      "4610/4610 [==============================] - 0s 30us/step - loss: 0.0093\n",
      "Epoch 46/50\n",
      "4610/4610 [==============================] - 0s 30us/step - loss: 0.0135\n",
      "Epoch 47/50\n",
      "4610/4610 [==============================] - 0s 29us/step - loss: 0.0117\n",
      "Epoch 48/50\n",
      "4610/4610 [==============================] - 0s 31us/step - loss: 0.0136\n",
      "Epoch 49/50\n",
      "4610/4610 [==============================] - 0s 31us/step - loss: 0.0176\n",
      "Epoch 50/50\n",
      "4610/4610 [==============================] - 0s 30us/step - loss: 0.0180\n",
      "Epoch 1/50\n",
      "4610/4610 [==============================] - 0s 47us/step - loss: -0.0224\n",
      "Epoch 2/50\n",
      "4610/4610 [==============================] - 0s 32us/step - loss: -0.0237\n",
      "Epoch 3/50\n",
      "4610/4610 [==============================] - 0s 32us/step - loss: -0.0261\n",
      "Epoch 4/50\n",
      "4610/4610 [==============================] - 0s 32us/step - loss: -0.0242\n",
      "Epoch 5/50\n",
      "4610/4610 [==============================] - 0s 32us/step - loss: -0.0226\n",
      "Epoch 6/50\n",
      "4610/4610 [==============================] - 0s 32us/step - loss: -0.0231\n",
      "Epoch 7/50\n",
      "4610/4610 [==============================] - 0s 33us/step - loss: -0.0232\n",
      "Epoch 8/50\n",
      "4610/4610 [==============================] - 0s 31us/step - loss: -0.0232\n",
      "Epoch 9/50\n",
      "4610/4610 [==============================] - 0s 33us/step - loss: -0.0211\n",
      "Epoch 10/50\n",
      "4610/4610 [==============================] - 0s 34us/step - loss: -0.0219\n",
      "Epoch 11/50\n",
      "4610/4610 [==============================] - 0s 32us/step - loss: -0.0243\n",
      "Epoch 12/50\n",
      "4610/4610 [==============================] - 0s 34us/step - loss: -0.0227\n",
      "Epoch 13/50\n",
      "4610/4610 [==============================] - 0s 32us/step - loss: -0.0228\n",
      "Epoch 14/50\n",
      "4610/4610 [==============================] - 0s 33us/step - loss: -0.0221\n",
      "Epoch 15/50\n",
      "4610/4610 [==============================] - 0s 33us/step - loss: -0.0237\n",
      "Epoch 16/50\n",
      "4610/4610 [==============================] - 0s 33us/step - loss: -0.0236\n",
      "Epoch 17/50\n",
      "4610/4610 [==============================] - 0s 33us/step - loss: -0.0227\n",
      "Epoch 18/50\n",
      "4610/4610 [==============================] - 0s 32us/step - loss: -0.0233\n",
      "Epoch 19/50\n",
      "4610/4610 [==============================] - 0s 33us/step - loss: -0.0223\n",
      "Epoch 20/50\n",
      "4610/4610 [==============================] - 0s 33us/step - loss: -0.0236\n",
      "Epoch 21/50\n",
      "4610/4610 [==============================] - 0s 33us/step - loss: -0.0230\n",
      "Epoch 22/50\n",
      "4610/4610 [==============================] - 0s 34us/step - loss: -0.0232\n",
      "Epoch 23/50\n",
      "4610/4610 [==============================] - 0s 33us/step - loss: -0.0233\n",
      "Epoch 24/50\n",
      "4610/4610 [==============================] - 0s 33us/step - loss: -0.0233\n",
      "Epoch 25/50\n",
      "4610/4610 [==============================] - 0s 32us/step - loss: -0.0232\n",
      "Epoch 26/50\n",
      "4610/4610 [==============================] - 0s 32us/step - loss: -0.0241\n",
      "Epoch 27/50\n",
      "4610/4610 [==============================] - 0s 34us/step - loss: -0.0235\n",
      "Epoch 28/50\n",
      "4610/4610 [==============================] - 0s 32us/step - loss: -0.0241\n",
      "Epoch 29/50\n",
      "4610/4610 [==============================] - 0s 33us/step - loss: -0.0226\n",
      "Epoch 30/50\n",
      "4610/4610 [==============================] - 0s 33us/step - loss: -0.0247\n",
      "Epoch 31/50\n",
      "4610/4610 [==============================] - 0s 34us/step - loss: -0.0229\n",
      "Epoch 32/50\n",
      "4610/4610 [==============================] - 0s 32us/step - loss: -0.0244\n",
      "Epoch 33/50\n",
      "4610/4610 [==============================] - 0s 32us/step - loss: -0.0230\n",
      "Epoch 34/50\n",
      "4610/4610 [==============================] - 0s 33us/step - loss: -0.0221\n",
      "Epoch 35/50\n",
      "4610/4610 [==============================] - 0s 34us/step - loss: -0.0225\n",
      "Epoch 36/50\n",
      "4610/4610 [==============================] - 0s 35us/step - loss: -0.0240\n",
      "Epoch 37/50\n",
      "4610/4610 [==============================] - 0s 34us/step - loss: -0.0222\n",
      "Epoch 38/50\n",
      "4610/4610 [==============================] - 0s 31us/step - loss: -0.0226\n",
      "Epoch 39/50\n",
      "4610/4610 [==============================] - 0s 35us/step - loss: -0.0244\n",
      "Epoch 40/50\n",
      "4610/4610 [==============================] - 0s 37us/step - loss: -0.0235\n",
      "Epoch 41/50\n",
      "4610/4610 [==============================] - 0s 33us/step - loss: -0.0235\n",
      "Epoch 42/50\n",
      "4610/4610 [==============================] - 0s 32us/step - loss: -0.0242\n",
      "Epoch 43/50\n",
      "4610/4610 [==============================] - 0s 33us/step - loss: -0.0224\n",
      "Epoch 44/50\n",
      "4610/4610 [==============================] - 0s 33us/step - loss: -0.0238\n",
      "Epoch 45/50\n",
      "4610/4610 [==============================] - 0s 34us/step - loss: -0.0228\n",
      "Epoch 46/50\n",
      "4610/4610 [==============================] - 0s 32us/step - loss: -0.0238\n",
      "Epoch 47/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4610/4610 [==============================] - 0s 33us/step - loss: -0.0237\n",
      "Epoch 48/50\n",
      "4610/4610 [==============================] - 0s 32us/step - loss: -0.0231\n",
      "Epoch 49/50\n",
      "4610/4610 [==============================] - 0s 32us/step - loss: -0.0224\n",
      "Epoch 50/50\n",
      "4610/4610 [==============================] - 0s 33us/step - loss: -0.0230\n",
      "Epoch 1/50\n",
      "4610/4610 [==============================] - 0s 58us/step - loss: 0.0140\n",
      "Epoch 2/50\n",
      "4610/4610 [==============================] - 0s 31us/step - loss: 0.0141\n",
      "Epoch 3/50\n",
      "4610/4610 [==============================] - 0s 32us/step - loss: 0.0139\n",
      "Epoch 4/50\n",
      "4610/4610 [==============================] - 0s 30us/step - loss: 0.0115\n",
      "Epoch 5/50\n",
      "4610/4610 [==============================] - 0s 30us/step - loss: 0.0279\n",
      "Epoch 6/50\n",
      "4610/4610 [==============================] - 0s 35us/step - loss: 0.0118\n",
      "Epoch 7/50\n",
      "4610/4610 [==============================] - 0s 32us/step - loss: 0.0122\n",
      "Epoch 8/50\n",
      "4610/4610 [==============================] - 0s 31us/step - loss: 0.0123\n",
      "Epoch 9/50\n",
      "4610/4610 [==============================] - 0s 31us/step - loss: 0.0118\n",
      "Epoch 10/50\n",
      "4610/4610 [==============================] - 0s 31us/step - loss: 0.0111\n",
      "Epoch 11/50\n",
      "4610/4610 [==============================] - 0s 32us/step - loss: 0.0123\n",
      "Epoch 12/50\n",
      "4610/4610 [==============================] - 0s 37us/step - loss: 0.0149\n",
      "Epoch 13/50\n",
      "4610/4610 [==============================] - 0s 33us/step - loss: 0.0146\n",
      "Epoch 14/50\n",
      "4610/4610 [==============================] - 0s 33us/step - loss: 0.0154\n",
      "Epoch 15/50\n",
      "4610/4610 [==============================] - 0s 35us/step - loss: 0.0125\n",
      "Epoch 16/50\n",
      "4610/4610 [==============================] - 0s 31us/step - loss: 0.0118\n",
      "Epoch 17/50\n",
      "4610/4610 [==============================] - 0s 32us/step - loss: 0.0110\n",
      "Epoch 18/50\n",
      "4610/4610 [==============================] - 0s 31us/step - loss: 0.0127\n",
      "Epoch 19/50\n",
      "4610/4610 [==============================] - 0s 30us/step - loss: 0.0142\n",
      "Epoch 20/50\n",
      "4610/4610 [==============================] - 0s 30us/step - loss: 0.0117\n",
      "Epoch 21/50\n",
      "4610/4610 [==============================] - 0s 31us/step - loss: 0.0145\n",
      "Epoch 22/50\n",
      "4610/4610 [==============================] - 0s 31us/step - loss: 0.0164\n",
      "Epoch 23/50\n",
      "4610/4610 [==============================] - 0s 32us/step - loss: 0.0149\n",
      "Epoch 24/50\n",
      "4610/4610 [==============================] - 0s 31us/step - loss: 0.0131\n",
      "Epoch 25/50\n",
      "4610/4610 [==============================] - 0s 32us/step - loss: 0.0165\n",
      "Epoch 26/50\n",
      "4610/4610 [==============================] - 0s 34us/step - loss: 0.0166\n",
      "Epoch 27/50\n",
      "4610/4610 [==============================] - 0s 31us/step - loss: 0.0111\n",
      "Epoch 28/50\n",
      "4610/4610 [==============================] - 0s 31us/step - loss: 0.0142\n",
      "Epoch 29/50\n",
      "4610/4610 [==============================] - 0s 31us/step - loss: 0.0188\n",
      "Epoch 30/50\n",
      "4610/4610 [==============================] - 0s 32us/step - loss: 0.0119\n",
      "Epoch 31/50\n",
      "4610/4610 [==============================] - 0s 31us/step - loss: 0.0129\n",
      "Epoch 32/50\n",
      "4610/4610 [==============================] - 0s 33us/step - loss: 0.0112\n",
      "Epoch 33/50\n",
      "4610/4610 [==============================] - 0s 33us/step - loss: 0.0147\n",
      "Epoch 34/50\n",
      "4610/4610 [==============================] - 0s 31us/step - loss: 0.0112\n",
      "Epoch 35/50\n",
      "4610/4610 [==============================] - 0s 32us/step - loss: 0.0135\n",
      "Epoch 36/50\n",
      "4610/4610 [==============================] - 0s 33us/step - loss: 0.0109\n",
      "Epoch 37/50\n",
      "4610/4610 [==============================] - 0s 35us/step - loss: 0.0137\n",
      "Epoch 38/50\n",
      "4610/4610 [==============================] - 0s 35us/step - loss: 0.0097\n",
      "Epoch 39/50\n",
      "4610/4610 [==============================] - 0s 31us/step - loss: 0.0134\n",
      "Epoch 40/50\n",
      "4610/4610 [==============================] - 0s 32us/step - loss: 0.0103\n",
      "Epoch 41/50\n",
      "4610/4610 [==============================] - 0s 31us/step - loss: 0.0222\n",
      "Epoch 42/50\n",
      "4610/4610 [==============================] - 0s 36us/step - loss: 0.0126\n",
      "Epoch 43/50\n",
      "4610/4610 [==============================] - 0s 32us/step - loss: 0.0127\n",
      "Epoch 44/50\n",
      "4610/4610 [==============================] - 0s 31us/step - loss: 0.0121\n",
      "Epoch 45/50\n",
      "4610/4610 [==============================] - 0s 32us/step - loss: 0.0268\n",
      "Epoch 46/50\n",
      "4610/4610 [==============================] - 0s 31us/step - loss: 0.0146\n",
      "Epoch 47/50\n",
      "4610/4610 [==============================] - 0s 31us/step - loss: 0.0132\n",
      "Epoch 48/50\n",
      "4610/4610 [==============================] - 0s 30us/step - loss: 0.0168\n",
      "Epoch 49/50\n",
      "4610/4610 [==============================] - 0s 32us/step - loss: 0.0128\n",
      "Epoch 50/50\n",
      "4610/4610 [==============================] - 0s 31us/step - loss: 0.0114\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,4):\n",
    "    \n",
    "    print(\"Loop: \"+str(i))\n",
    "    \n",
    "    # import Dataset\n",
    "    # pull random draws\n",
    "    conn = psycopg2.connect(\"dbname=Kelly_2017_2020 user=postgres password=password\")\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    cur.execute(\"SELECT * FROM data TABLESAMPLE BERNOULLI(10);\")\n",
    "    train_batch = cur.fetchall()\n",
    "    print(\"got data\")\n",
    "\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "    train_batch = pd.DataFrame(train_batch, columns = pd.DataFrame(column_names)[0])\n",
    "    \n",
    "    \n",
    "    #preprocess\n",
    "    train_batch.replace(\"EMPTY\", np.nan, inplace = True)\n",
    "    \n",
    "    # drop identifiers, cols with lots of nan and drop nan Y values\n",
    "    train_batch.drop(columns=identifiers, inplace = True)\n",
    "    train_batch = train_batch[False == train_batch[\"ret_exc_lead1m\"].isnull()]\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    # not in loop, only once! define delete_cols after text conversion\n",
    "    #used once only\n",
    "    #train_batch.loc[:,train_batch.isna().sum()>train_batch.isna().sum().quantile(0.1)].columns.values\n",
    "\n",
    "    \n",
    "    #np.savetxt(\"Networks\\delete_cols.csv\", delete_cols, delimiter=\",\")\n",
    "    # join macro\n",
    "    train_batch = pd.merge(train_batch,macro_data, on =\"eom\")\n",
    "    train_batch.drop(columns=[\"eom\"], inplace = True)\n",
    "\n",
    "    \n",
    "    # for the whole dataset the same columns should be dropped\n",
    "    #train_batch.drop(columns=delete_cols, inplace = True)\n",
    "    #val_batch.drop(columns=delete_cols, inplace = True)\n",
    "    \n",
    "    # for the whole dataset the same columns should be dropped\n",
    "    train_batch.drop(columns=delete_cols, inplace = True)\n",
    "    \n",
    "\n",
    "    # choose this or fillna: drop all rows with nan entries\n",
    "    train_batch=train_batch.loc[False==train_batch.isna().any(axis=1),:]\n",
    "    \n",
    "    \n",
    "    # percentile scaling before median filling\n",
    "    X_train_batch=quantile_transform(np.array(train_batch.drop(columns=['ret_exc_lead1m'])), n_quantiles=1000)\n",
    "    # transform back to DF to make fillna possible\n",
    "    X_train_batch=pd.DataFrame(X_train_batch)\n",
    "    Y_train_batch = train_batch[\"ret_exc_lead1m\"].astype(float)\n",
    "    \n",
    "\n",
    "    # fill with median or drop?? drop a mix of cols and rows??\n",
    "    #X_train_batch.fillna(X_train_batch.median(), inplace=True)\n",
    "    \n",
    "\n",
    "    # alternative to fillna(): dropna\n",
    "    # X_train_batch.drop(train_batch.isna().any(axis=1),axis = 0, inplace=True)\n",
    "    # X_val_batch.drop(train_batch.isna().any(axis=1),axis = 0, inplace=True)\n",
    "    \n",
    "    # optimizer\n",
    "    batch_size = 200\n",
    "    num_epochs = 50\n",
    "    opt=Adamax(lr=0.001)\n",
    "    # turn on discriminator\n",
    "    network.layers[1].trainable = True\n",
    "    network.layers[2].trainable = True\n",
    "    network.layers[3].trainable = True\n",
    "    network.layers[5].trainable = True\n",
    "    network.layers[7].trainable = True\n",
    "\n",
    "    # turn of adverse\n",
    "    network.layers[4].trainable = False\n",
    "    network.layers[6].trainable = False\n",
    "\n",
    "    network.compile(optimizer=opt,\\\n",
    "                       loss=custom_loss)\n",
    "    history=network.fit(X_train_batch, Y_train_batch, \\\n",
    "                        epochs=num_epochs, batch_size=batch_size)\n",
    "    \n",
    "    # Adversarial\n",
    "    # turn of discriminator\n",
    "    network.layers[1].trainable = False\n",
    "    network.layers[2].trainable = False\n",
    "    network.layers[3].trainable = False\n",
    "    network.layers[5].trainable = False\n",
    "    network.layers[7].trainable = False\n",
    "\n",
    "    # turn on adverse\n",
    "    network.layers[4].trainable = True\n",
    "    network.layers[6].trainable = True\n",
    "    opt=SGD(lr=0.01)\n",
    "    network.compile(optimizer=opt,\\\n",
    "                       loss=custom_loss_adverse)\n",
    "    history=network.fit(X_train_batch, Y_train_batch, epochs=num_epochs, batch_size=128)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # optimizer again\n",
    "    opt=Adamax(lr=0.001)\n",
    "    # turn on discriminator\n",
    "    network.layers[1].trainable = True\n",
    "    network.layers[2].trainable = True\n",
    "    network.layers[3].trainable = True\n",
    "    network.layers[5].trainable = True\n",
    "    network.layers[7].trainable = True\n",
    "\n",
    "    # turn of adverse\n",
    "    network.layers[4].trainable = False\n",
    "    network.layers[6].trainable = False\n",
    "\n",
    "    network.compile(optimizer=opt,\\\n",
    "                       loss=custom_loss)\n",
    "    history=network.fit(X_train_batch, Y_train_batch, \\\n",
    "                        epochs=num_epochs, batch_size=batch_size)\n",
    "                        #callbacks=[CustomCallback()], \\\n",
    "                        #validation_data=(X_val_batch, Y_val_batch))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.save('Networks\\prototype_03.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99482024"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=network.predict(X_train_batch)\n",
    "y_pred[:,0].min()/y_pred[:,0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9332940108001208"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1-w*R is the SDF\n",
    "(1-sum(y_pred[:,0]*Y_train_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01690789458299724"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss1\n",
    "# M * R\n",
    "# (1-w *R) * R \n",
    "w=network.predict(X_train_batch)\n",
    "np.mean(((1-sum(w[:,0]*Y_train_batch)))*Y_train_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 285., 1531., 1196.,  743.,  481.,  216.,  101.,   40.,   11.,\n",
       "           6.]),\n",
       " array([0.00079715, 0.00079757, 0.00079798, 0.0007984 , 0.00079881,\n",
       "        0.00079923, 0.00079964, 0.00080006, 0.00080047, 0.00080089,\n",
       "        0.0008013 ], dtype=float32),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(w[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.016907876549368282"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss2\n",
    "# M * R * G\n",
    "# (1-w *R) * R * G\n",
    "w=network.predict(X_train_batch)\n",
    "np.mean(((1-sum(w[:,0]*Y_train_batch)) )*Y_train_batch*w[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>481</th>\n",
       "      <th>482</th>\n",
       "      <th>483</th>\n",
       "      <th>484</th>\n",
       "      <th>485</th>\n",
       "      <th>486</th>\n",
       "      <th>487</th>\n",
       "      <th>488</th>\n",
       "      <th>489</th>\n",
       "      <th>490</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.955956</td>\n",
       "      <td>0.939835</td>\n",
       "      <td>0.939118</td>\n",
       "      <td>0.680805</td>\n",
       "      <td>0.680805</td>\n",
       "      <td>0.700464</td>\n",
       "      <td>0.703253</td>\n",
       "      <td>0.940635</td>\n",
       "      <td>0.936338</td>\n",
       "      <td>0.167615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.319820</td>\n",
       "      <td>0.335836</td>\n",
       "      <td>0.633133</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.942442</td>\n",
       "      <td>0.439439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.668168</td>\n",
       "      <td>0.392893</td>\n",
       "      <td>0.297798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.025199</td>\n",
       "      <td>0.112005</td>\n",
       "      <td>0.111096</td>\n",
       "      <td>0.487507</td>\n",
       "      <td>0.487507</td>\n",
       "      <td>0.470883</td>\n",
       "      <td>0.505012</td>\n",
       "      <td>0.105762</td>\n",
       "      <td>0.055216</td>\n",
       "      <td>0.522074</td>\n",
       "      <td>...</td>\n",
       "      <td>0.319820</td>\n",
       "      <td>0.335836</td>\n",
       "      <td>0.633133</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.942442</td>\n",
       "      <td>0.439439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.668168</td>\n",
       "      <td>0.392893</td>\n",
       "      <td>0.297798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.364618</td>\n",
       "      <td>0.509344</td>\n",
       "      <td>0.506680</td>\n",
       "      <td>0.667790</td>\n",
       "      <td>0.667790</td>\n",
       "      <td>0.670671</td>\n",
       "      <td>0.692196</td>\n",
       "      <td>0.438901</td>\n",
       "      <td>0.313188</td>\n",
       "      <td>0.289286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.319820</td>\n",
       "      <td>0.335836</td>\n",
       "      <td>0.633133</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.942442</td>\n",
       "      <td>0.439439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.668168</td>\n",
       "      <td>0.392893</td>\n",
       "      <td>0.297798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.600104</td>\n",
       "      <td>0.754201</td>\n",
       "      <td>0.752495</td>\n",
       "      <td>0.817787</td>\n",
       "      <td>0.817787</td>\n",
       "      <td>0.827259</td>\n",
       "      <td>0.834909</td>\n",
       "      <td>0.706725</td>\n",
       "      <td>0.532216</td>\n",
       "      <td>0.168796</td>\n",
       "      <td>...</td>\n",
       "      <td>0.319820</td>\n",
       "      <td>0.335836</td>\n",
       "      <td>0.633133</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.942442</td>\n",
       "      <td>0.439439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.668168</td>\n",
       "      <td>0.392893</td>\n",
       "      <td>0.297798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.256251</td>\n",
       "      <td>0.355064</td>\n",
       "      <td>0.353086</td>\n",
       "      <td>0.505506</td>\n",
       "      <td>0.505506</td>\n",
       "      <td>0.517626</td>\n",
       "      <td>0.533033</td>\n",
       "      <td>0.399981</td>\n",
       "      <td>0.355329</td>\n",
       "      <td>0.179766</td>\n",
       "      <td>...</td>\n",
       "      <td>0.319820</td>\n",
       "      <td>0.335836</td>\n",
       "      <td>0.633133</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.942442</td>\n",
       "      <td>0.439439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.668168</td>\n",
       "      <td>0.392893</td>\n",
       "      <td>0.297798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4605</th>\n",
       "      <td>0.408520</td>\n",
       "      <td>0.100088</td>\n",
       "      <td>0.099074</td>\n",
       "      <td>0.055140</td>\n",
       "      <td>0.055140</td>\n",
       "      <td>0.053991</td>\n",
       "      <td>0.061061</td>\n",
       "      <td>0.153951</td>\n",
       "      <td>0.388569</td>\n",
       "      <td>0.105793</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068569</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.926927</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.239239</td>\n",
       "      <td>0.147147</td>\n",
       "      <td>0.549049</td>\n",
       "      <td>0.805305</td>\n",
       "      <td>0.748248</td>\n",
       "      <td>0.147147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4606</th>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.626257</td>\n",
       "      <td>0.624316</td>\n",
       "      <td>0.536227</td>\n",
       "      <td>0.536227</td>\n",
       "      <td>0.520967</td>\n",
       "      <td>0.546531</td>\n",
       "      <td>0.692640</td>\n",
       "      <td>0.728856</td>\n",
       "      <td>0.581217</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068569</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.926927</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.239239</td>\n",
       "      <td>0.147147</td>\n",
       "      <td>0.549049</td>\n",
       "      <td>0.805305</td>\n",
       "      <td>0.748248</td>\n",
       "      <td>0.147147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4607</th>\n",
       "      <td>0.471909</td>\n",
       "      <td>0.672309</td>\n",
       "      <td>0.669996</td>\n",
       "      <td>0.814544</td>\n",
       "      <td>0.814544</td>\n",
       "      <td>0.798404</td>\n",
       "      <td>0.828129</td>\n",
       "      <td>0.699171</td>\n",
       "      <td>0.522599</td>\n",
       "      <td>0.476626</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068569</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.926927</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.239239</td>\n",
       "      <td>0.147147</td>\n",
       "      <td>0.549049</td>\n",
       "      <td>0.805305</td>\n",
       "      <td>0.748248</td>\n",
       "      <td>0.147147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4608</th>\n",
       "      <td>0.439410</td>\n",
       "      <td>0.551606</td>\n",
       "      <td>0.550080</td>\n",
       "      <td>0.647831</td>\n",
       "      <td>0.647831</td>\n",
       "      <td>0.624559</td>\n",
       "      <td>0.649445</td>\n",
       "      <td>0.426744</td>\n",
       "      <td>0.317138</td>\n",
       "      <td>0.732179</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068569</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.926927</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.239239</td>\n",
       "      <td>0.147147</td>\n",
       "      <td>0.549049</td>\n",
       "      <td>0.805305</td>\n",
       "      <td>0.748248</td>\n",
       "      <td>0.147147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4609</th>\n",
       "      <td>0.182561</td>\n",
       "      <td>0.069740</td>\n",
       "      <td>0.068695</td>\n",
       "      <td>0.087000</td>\n",
       "      <td>0.087000</td>\n",
       "      <td>0.080080</td>\n",
       "      <td>0.096141</td>\n",
       "      <td>0.064623</td>\n",
       "      <td>0.120237</td>\n",
       "      <td>0.366661</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068569</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.926927</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.239239</td>\n",
       "      <td>0.147147</td>\n",
       "      <td>0.549049</td>\n",
       "      <td>0.805305</td>\n",
       "      <td>0.748248</td>\n",
       "      <td>0.147147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4610 rows Ã— 491 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6    \\\n",
       "0     0.955956  0.939835  0.939118  0.680805  0.680805  0.700464  0.703253   \n",
       "1     0.025199  0.112005  0.111096  0.487507  0.487507  0.470883  0.505012   \n",
       "2     0.364618  0.509344  0.506680  0.667790  0.667790  0.670671  0.692196   \n",
       "3     0.600104  0.754201  0.752495  0.817787  0.817787  0.827259  0.834909   \n",
       "4     0.256251  0.355064  0.353086  0.505506  0.505506  0.517626  0.533033   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4605  0.408520  0.100088  0.099074  0.055140  0.055140  0.053991  0.061061   \n",
       "4606  0.650000  0.626257  0.624316  0.536227  0.536227  0.520967  0.546531   \n",
       "4607  0.471909  0.672309  0.669996  0.814544  0.814544  0.798404  0.828129   \n",
       "4608  0.439410  0.551606  0.550080  0.647831  0.647831  0.624559  0.649445   \n",
       "4609  0.182561  0.069740  0.068695  0.087000  0.087000  0.080080  0.096141   \n",
       "\n",
       "           7         8         9    ...       481       482       483  484  \\\n",
       "0     0.940635  0.936338  0.167615  ...  0.319820  0.335836  0.633133  0.0   \n",
       "1     0.105762  0.055216  0.522074  ...  0.319820  0.335836  0.633133  0.0   \n",
       "2     0.438901  0.313188  0.289286  ...  0.319820  0.335836  0.633133  0.0   \n",
       "3     0.706725  0.532216  0.168796  ...  0.319820  0.335836  0.633133  0.0   \n",
       "4     0.399981  0.355329  0.179766  ...  0.319820  0.335836  0.633133  0.0   \n",
       "...        ...       ...       ...  ...       ...       ...       ...  ...   \n",
       "4605  0.153951  0.388569  0.105793  ...  0.068569  0.000000  0.926927  0.0   \n",
       "4606  0.692640  0.728856  0.581217  ...  0.068569  0.000000  0.926927  0.0   \n",
       "4607  0.699171  0.522599  0.476626  ...  0.068569  0.000000  0.926927  0.0   \n",
       "4608  0.426744  0.317138  0.732179  ...  0.068569  0.000000  0.926927  0.0   \n",
       "4609  0.064623  0.120237  0.366661  ...  0.068569  0.000000  0.926927  0.0   \n",
       "\n",
       "           485       486       487       488       489       490  \n",
       "0     0.942442  0.439439  0.000000  0.668168  0.392893  0.297798  \n",
       "1     0.942442  0.439439  0.000000  0.668168  0.392893  0.297798  \n",
       "2     0.942442  0.439439  0.000000  0.668168  0.392893  0.297798  \n",
       "3     0.942442  0.439439  0.000000  0.668168  0.392893  0.297798  \n",
       "4     0.942442  0.439439  0.000000  0.668168  0.392893  0.297798  \n",
       "...        ...       ...       ...       ...       ...       ...  \n",
       "4605  0.239239  0.147147  0.549049  0.805305  0.748248  0.147147  \n",
       "4606  0.239239  0.147147  0.549049  0.805305  0.748248  0.147147  \n",
       "4607  0.239239  0.147147  0.549049  0.805305  0.748248  0.147147  \n",
       "4608  0.239239  0.147147  0.549049  0.805305  0.748248  0.147147  \n",
       "4609  0.239239  0.147147  0.549049  0.805305  0.748248  0.147147  \n",
       "\n",
       "[4610 rows x 491 columns]"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.186228046304332"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Return_List = []\n",
    "for i in range(0,90):\n",
    "    w=network.predict(X_train_batch.iloc[0+i*50:50+i*50])\n",
    "    Return_List.append(sum(w[:,0]*Y_train_batch.iloc[0+i*50:50+i*50]))\n",
    "(np.array(Return_List).mean()/np.array(Return_List).std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2632568065333715"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Return_List_BH = []\n",
    "for i in range(0,18):\n",
    "    Return_List_BH.append(Y_train_batch.iloc[0+i*500:500+i*500])\n",
    "\n",
    "\n",
    "np.array(Return_List_BH)[0].mean()/np.array(Return_List_BH)[0].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSCI monthly sharpe\n",
    "0.7/np.sqrt(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08501535852639515"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sharpe... maybe not transferable to \"invest once\", need portfolio\n",
    "(w[:,0]*Y_val_batch).mean()/(w[:,0]*Y_val_batch).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_val_batch.mean()/Y_val_batch.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /w[:,0].sum() to adjust for weight sum <1 -> ptf of 100%\n",
    "((w[:,0]/w[:,0].sum())*y_features_set).sum()\n",
    "\n",
    "# annualized\n",
    "(((w[:,0]/w[:,0].sum())*y_features_set).sum()+1)**3-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return\n",
    "print((w[:,0]*Y_val_batch).sum())\n",
    "\n",
    "# sum of weights\n",
    "print(w[:,0].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(acc[0]) + 1)\n",
    "colorlist = ['blue','cyan','purple','pink','orange','red','brown','gray','olive','green']\n",
    "plt.figure(figsize=(12,6))\n",
    "i=0\n",
    "while i <  len(acc):\n",
    "    plt.plot(epochs, acc[i] , 'bo', label='Training acc', color = colorlist[i])\n",
    "    plt.plot(epochs, val_acc[i], 'b', label='Validation acc', color=colorlist[i])\n",
    "    \n",
    "    ax= plt.gca()\n",
    "    ax.tick_params(direction='in')\n",
    "    i+=1\n",
    "#plt.ylim(ymax = 1, ymin = 0.6)\n",
    "plt.title('Training and validation accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weights = network.predict(X)[:,0]\n",
    "g_i = network.predict(X)[:,1]\n",
    "excess_returns = Y\n",
    "(weights * excess_returns).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload.predict(X_val_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.predict(X_val_batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
